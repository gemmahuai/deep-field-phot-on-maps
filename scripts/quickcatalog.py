"""

generates simulated SPHEREx Level 3 Catalog data bypassing spectral images and forced photometry.

@author: bcrill

Gemma Huai modified on 01/19/2025, background subtraction
quickcatalog_bkg_rmv.py
"""

import copy
import logging

import astropy.units as u
import numpy as np
import pyarrow as pa
import SPHEREx_InstrumentSimulator as SPinst
import SPHEREx_ObsSimulator as SPobs
import SPHEREx_SkySimulator as SPsky
from astropy.coordinates import ICRS, SkyCoord, get_body
from astropy.io import fits
from astropy.table import Table, vstack
from astropy.time import Time
from numpy.polynomial.polynomial import Polynomial
from pyarrow import parquet
from scipy import interpolate
from skimage.transform import downscale_local_mean
from SPHEREx_Simulator_Tools import SPHEREx_Logger, data_filename
import time
from matplotlib import pyplot as plt

try:
    import astrometry
    from SPHEREx_SkySimulator import tractor_utils
    from tractor import WCS
    from tractor.galaxy import *

    tractor_is_installed = True
except ModuleNotFoundError:
    tractor_is_installed = False


def save_level3_primary(
    sim_catalog,
    output_path,
    sim_truth=None,
    pointing_table=None,
    spherex_parameters=None,
    source_id_lookup=None,
    external_survey_lookup=None,
    external_source_id_lookup=None,
    spherex_class_lookup=None,
    pm_ra=None,
    pm_dec=None,
    parallax=None,
):
    """Save `QuickCatalog` output to parquet (primary All Sky Catalog)

    Notes
    -----
    This function saves a catalog generated by
    `~SPHEREx_SkySimulator.quickcatalog.QuickCatalog` or
    `~SPHEREx_SkySimulator.quicker_catalog.QuickerCatalog` so that it looks
    like a primary All Sky Catalog in a parquet file in the format specified in
    "SSDC Data Product Specification: Level 3 All-Sky Catalog" Version 1.0
    February 13, 2023

    Parameters
    ----------
    sim_catalog : `~astropy.Table`
        A table object containing the catalog data.

    output_path : str or None
        The path and filename to write the output parquet file.
        If `None`, the function will return the pyarrow table.

    sim_truth : `~astropy.Table`, optional
        The truth catalog output by
        `~SPHEREx_SkySimulator.quickcatalog.QuickCatalog`. The zodi background
        is retrieved and included in the output parquet file.
        Default is `None`.

    pointing_table : `~astropy.Table`, optional
        Survey plan used to generate the catalog.
        Default is `None`.

    spherex_parameters : dict, optional
        A SPHEREx parameter dictionary.
        Default is `None`.

    source_id_lookup : dict, optional
        A dictionary that provides a translation between the ``"SOURCE_ID"``
        field produced by `~SPHEREx_SkySimulator.quickcatalog.QuickCatalog` and
        the output ``"source_id"``.
        Default is `None`.

    external_survey_lookup : dict, optional
        A dictionary that provides a translation between the ``"SOURCE_ID"``
        field produced by `~SPHEREx_SkySimulator.quickcatalog.QuickCatalog` and
        the output ``"external_survey"``.
        Default is `None`.

    spherex_class_lookup : dict, optional
        A dictionary that provides a translation between the ``"SOURCE_ID"``
        field produced by `~SPHEREx_SkySimulator.quickcatalog.QuickCatalog` and
        the output ``"spherex_class"``.
        Default is `None`.

        .. note::
            ``"spherex_class"`` is a bitwise flag with information about what
            the source is used for by the pipeline. ::

                    Bit Name
                    0 ASTROMETRIC
                    1 PSF
                    2 PHOTOMETRIC
                    3 CALSPEC
                    4 BRIGHT
                    5 COSMOLOGY
                    6 ICES
    pm_ra, pm_dec, parallax : float, array-like, optional
        Proper motion (ra and dec) and parallax data to include in the output
        file.
        Default is `None`.

    Returns:
        None
    """
    if not isinstance(sim_catalog, Table):
        raise TypeError("input parameter `Catalog_Table` not an astropy.Table object")

    # Define lists of types for parquet library
    lfloat64 = pa.list_(pa.float64())
    lfloat32 = pa.list_(pa.float32())
    lbool = pa.list_(pa.bool_())
    lint64 = pa.list_(pa.int64())
    lint8 = pa.list_(pa.int8())

    #
    # the scheme for the parquet file output.
    # the tuple items are name, parquet data type, units, description
    #

    pa_flds_info = [
        ("source_id", pa.int64(), None, "Unique SPHEREx ID"),
        ("external_survey", pa.int8(), None, "Identifier of external survey"),
        (
            "external_source_id",
            pa.int64(),
            None,
            "ID of source in external_survey catalog",
        ),
        ("spherex_class", pa.int32(), None, "Source origin in reference catalog"),
        ("ra", pa.float64(), u.deg, "R.A. (from reference catalog)"),
        ("dec", pa.float64(), u.deg, "Decl. (from reference catalog)"),
        (
            "pm_ra",
            pa.float32(),
            u.mas / u.yr,
            "Proper motion in R.A. (from reference catalog)",
        ),
        (
            "pm_dec",
            pa.float32(),
            u.mas / u.yr,
            "Proper motion in Decl. (from reference catalog)",
        ),
        ("parallax", pa.float32(), u.mas, "Parallax (from reference catalog)"),
        ("x_image", lfloat32, None, "X pixel position on LVF image used for fitting"),
        ("y_image", lfloat32, None, "Y pixel position on LVF image used for fitting"),
        (
            "offset_ra",
            lfloat32,
            u.mas,
            "Delta R.A. offset from reference to fit position",
        ),
        (
            "offset_dec",
            lfloat32,
            u.mas,
            "Delta Decl. offset from reference to fit position",
        ),
        ("mjd", lfloat64, u.day, "Modified Julian Date of exposure midpoint"),
        ("lambda", lfloat32, u.micron, "SPHEREx wavelength"),
        ("lambda_width", lfloat32, u.micron, "SPHEREx wavelength width"),
        ("flux", lfloat32, u.uJy, "SPHEREx observed flux"),
        ("flux_err", lfloat32, u.uJy, "SPHEREx observed flux error"),
        (
            "flux_bkg",
            lfloat32,
            u.MJy / u.sr,
            "SPHEREx background surface brightness at source position",
        ),
        ("ext_flg", pa.bool_(), None, "True if treated as extended source"),
        ("flags", lint64, None, "Photometry flags"),
        ("fit_ql", lfloat32, None, "Photometry metric"),
        ("deep_flg", lbool, None, "True if Deep Field observation"),
        ("lvf_id", lint64, None, "ID of LVF exposure used for measurement"),
        ("det_id", lint8, None, "Detector Number"),
        ("nyquist_comp", pa.float32(), None, "Nyquist completeness"),
    ]
    pa_units = {}
    for t in pa_flds_info:
        pa_units[t[0]] = t[2]

    # these are output fields that are array-like
    list_items = (
        "x_image",
        "y_image",
        "offset_ra",
        "offset_dec",
        "mjd",
        "lambda",
        "lambda_width",
        "flux",
        "flux_err",
        "flux_bkg",
        "flags",
        "fit_ql",
        "deep_flg",
        "lvf_id",
        "det_id",
    )

    # these are output fields that are just a direct copy of the input
    direct_mapping = {
        "x_image": "X",
        "y_image": "Y",
        "flux": "FLUX",
        "flux_err": "FLUX_ERR",
        "lambda": "WAVELENGTH",
        "det_id": "DETECTOR",
    }
    convert = {
        "x_image": None,
        "y_image": None,
        "flux": u.uJy,
        "flux_err": u.uJy,
        "lambda": u.micron,
        "det_id": None,
    }

    sources = np.unique(sim_catalog["SOURCE_ID"])

    source_num = source_id_lookup or dict(zip(sources, np.arange(len(sources))))

    source_idx = np.array([source_num[i] for i in sim_catalog["SOURCE_ID"]])

    if spherex_parameters is not None:
        R = np.array(spherex_parameters["filter"]["R"]["value"])
    else:
        R = [41.2, 41.7, 41.1, 34.6, 117.1, 132.8]

    lambda_width = np.zeros_like(sim_catalog["WAVELENGTH"])
    for ii, wl in enumerate(sim_catalog["WAVELENGTH"]):
        lambda_width[ii] = wl / R[sim_catalog["DETECTOR"][ii] - 1]

    if sim_truth is not None:
        background = sim_truth["Zodi"].data
    else:
        background = np.zeros_like(sim_catalog["WAVELENGTH"])

    rows = []
    # loop over sources
    for isrc in np.unique(source_idx):
        # assemble the row in the All Sky Catalog
        row = {}

        # start with data that is just one value per row
        row["source_id"] = isrc

        if external_survey_lookup is not None:
            row["external_survey"] = external_survey_lookup[isrc]
        else:
            row["external_survey"] = 0

        if external_source_id_lookup is not None:
            row["external_source_id"] = external_source_id_lookup[isrc]
        else:
            row["external_source_id"] = 0

        # assign appropriate spherex_class for cosmology and ices:
        # Bit Name
        #   0 ASTROMETRIC
        #   1 PSF
        #   2 PHOTOMETRIC
        #   3 CALSPEC
        #   4 BRIGHT
        #   5 COSMOLOGY
        #   6 ICES
        if spherex_class_lookup is not None:
            row["spherex_class"] = spherex_class_lookup[isrc]
        else:
            row["spherex_class"] = 0

        row["pm_ra"] = pm_ra or 0.0
        row["pm_dec"] = pm_dec or 0.0
        row["parallax"] = parallax or 0.0
        row["ext_flg"] = False

        # now do rows that contain lists of data

        # pick out rows containing this source
        i = source_idx == isrc
        source_bg = background[i]
        source_lambda_width = lambda_width[i]
        mean_ra = np.average(sim_catalog["RA"][i])
        mean_dec = np.average(sim_catalog["DEC"][i])

        row["nyquist_comp"] = (len(source_bg) >= 2 * 105) * 1.0

        row["ra"] = mean_ra
        row["dec"] = mean_dec

        for col in list_items:
            row[col] = []

        # loop over observations of the given source
        for ii, input_row in enumerate(sim_catalog[i]):
            # get data that is just a one-to-one map
            # and convert units
            for col in direct_mapping.keys():
                value = input_row[direct_mapping[col]]
                if convert[col] is not None:
                    if not hasattr(value, "unit"):
                        value <<= sim_catalog[direct_mapping[col]].unit
                    value = value.to_value(pa_units[col])

                row[col].append(value)

            row["lvf_id"].append(input_row["IMAGE_ID"] * 10 + input_row["DETECTOR"])
            # convert to Modified Julian Date
            row["mjd"].append(input_row["JD"] - 2400000.5)

            # do we have the background data from the truth catalog?
            if sim_truth is not None:
                row["flux_bkg"].append(source_bg[ii])
            else:
                row["flux_bkg"].append(0.0)

            row["offset_ra"].append(mean_ra - input_row["RA"])
            row["offset_dec"].append(mean_dec - input_row["DEC"])

            row["lambda_width"].append(source_lambda_width[ii])

            in_deep = False
            if pointing_table is not None:
                in_deep = pointing_table[input_row["IMAGE_ID"]]["Flag"] in [
                    "deep_north",
                    "deep_south",
                ]

            row["deep_flg"].append(in_deep)

            row["flags"].append(0)
            row["fit_ql"].append(1)

        rows.append(row)

    # function to create field with metadata
    def create_pa_fld(name, pa_type, unit, descr):
        # all columns should have description
        fld_metadata = {"description": descr}
        if unit is not None:
            fld_metadata["unit"] = str(unit)
        return pa.field(name, pa_type, metadata=fld_metadata)

    schema = pa.schema([create_pa_fld(t[0], t[1], t[2], t[3]) for t in pa_flds_info])

    # Create pyarrow table with schema and rows
    allsky_cat = pa.Table.from_pylist(rows, schema=schema)

    if output_path is None:  # Do not save file but return pyarrow table
        return allsky_cat

    with parquet.ParquetWriter(output_path, schema=allsky_cat.schema) as writer:
        writer.write_table(allsky_cat)

    return


def save_level3_secondary(
    Primary_Catalog,
    Channels,
    Instrument,
    output_path,
    method="mean",
    pointing_table=None,
    source_id_lookup=None,
    external_survey_lookup=None,
    external_source_id_lookup=None,
    spherex_class_lookup=None,
    pm_ra=None,
    pm_dec=None,
    parallax=None,
    fluxerr_from_weights=False,
):
    if method not in ["mean", "spline", "poly"]:
        raise ValueError(f"Unsupported interpolated method: {method}")

    # set up synthetic bands
    synbands = [
        "F784",
        "LSST_y",
        "LSST_z",
        "2MASS_J",
        "2MASS_H",
        "2MASS_Ks",
        "WISE_W1",
        "WISE_W2",
    ]
    synband_interpolator = {}
    for synband in synbands:
        T = Table.read(
            data_filename(
                f"data/synflux_bands/{synband}.ecsv", package="SPHEREx_SkySimulator"
            )
        )
        synband_interpolator[synband] = interpolate.interp1d(
            T["wl"].data, T["t"].data, fill_value=0.0, bounds_error=False
        )

    # Define lists of types for parquet library
    lfloat32 = pa.list_(pa.float32())
    lbool = pa.list_(pa.bool_())
    lint64 = pa.list_(pa.int64())

    #
    # the scheme for the parquet file output.
    # the tuple items are name, parquet data type, units, description
    #

    pa_flds_info = [
        ("source_id", pa.int64(), None, "Unique SPHEREx ID"),
        ("external_survey", pa.int8(), None, "Identifier of external survey"),
        (
            "external_source_id",
            pa.int64(),
            None,
            "ID of source in external_survey catalog",
        ),
        ("spherex_class", pa.int32(), None, "Source origin in reference catalog"),
        ("ra", pa.float64(), u.deg, "R.A. (from reference catalog)"),
        ("dec", pa.float64(), u.deg, "Decl. (from reference catalog)"),
        (
            "pm_ra",
            pa.float32(),
            u.mas / u.yr,
            "Proper motion in R.A. (from reference catalog)",
        ),
        (
            "pm_dec",
            pa.float32(),
            u.mas / u.yr,
            "Proper motion in Decl. (from reference catalog)",
        ),
        ("parallax", pa.float32(), u.mas, "Parallax (from reference catalog)"),
        ("lambda", lfloat32, u.micron, "Fixed wavelength grid"),
        (
            "flux_allsky",
            lfloat32,
            u.uJy,
            "Derived flux at each interpolated wavelength.  Obtained from the AllSky survey tiles.",
        ),
        (
            "flux_err_allsky",
            lfloat32,
            u.uJy,
            "Derived flux uncertainty at each interpolated wavelength.  Obtained from the AllSky survey tiles.",
        ),
        (
            "flux_deepfield",
            lfloat32,
            u.uJy,
            "Derived flux at each interpolated wavelength.  Obtained from Deep Fields survey tiles.",
        ),
        (
            "flux_err_deepfield",
            lfloat32,
            u.uJy,
            "Derived flux uncertainty at each interpolated wavelength.  Obtained from Deep Fields survey tiles.",
        ),
        ("ext_flg", pa.bool_(), None, "True if treated as extended source"),
        (
            "unobs_flg_allsky",
            lbool,
            None,
            "Flag indicating if the source was not observed at a given interpolated wavelength.",
        ),
        (
            "unobs_flg_deepfield",
            lbool,
            None,
            "Flag indicating if the source was not observed at a given interpolated wavelength.",
        ),
        ("ql_fl", lint64, None, "Quality flag for the derived flux."),
        ("nyquist_comp", pa.float32(), None, "Nyquist completeness"),
        (
            "synflux",
            lfloat32,
            u.uJy,
            "Measured flux in F784, Rubin z and Y, 2MASS J, H, and Ks, and WISE W1 and W2 bands.",
        ),
        (
            "synflux_err",
            lfloat32,
            u.uJy,
            "Measured flux error in F784, Rubin z and Y, 2MASS J, H, and Ks, and WISE W1 and W2 bands.",
        ),
        ("var_flag", lbool, None, "Flag for variable sources."),
        (
            "var_ext_bright",
            lfloat32,
            None,
            "Additional flag for bright variable sources: contains sigma of variability N=102.",
        ),
        (
            "var_ext_faint",
            lfloat32,
            None,
            "Additional flag for faint variable sources: contains sigma of variability in synthetic bands.",
        ),
    ]

    pa_units = {}
    for t in pa_flds_info:
        pa_units[t[0]] = t[2]

    list_items = (
        "lambda",
        "flux_allsky",
        "flux_err_allsky",
        "flux_deepfield",
        "flux_err_deepfield",
        "unobs_flg_allsky",
        "unobs_flg_deepfield",
        "ql_fl",
        "synflux",
        "synflux_err",
        "var_flag",
        "var_ext_bright",
        "var_ext_faint",
    )

    # get center of band
    Channels["lambda"] = 0.5 * (Channels["lambda_min"] + Channels["lambda_max"])

    # get bandpass at center of a line of pixels
    py = np.arange(2048)
    wl_by_py = {}
    for array in [1, 2, 3, 4, 5, 6]:
        wl_by_py[array], tmp = Instrument.pixel_to_band(
            1024 * np.ones_like(py), py, central_bandpass_only=True, array=array
        )

    # get fiducial bandpass at secondary catalog grid center
    fiducial_bandpass = {}
    output_lambdas = []
    for row in Channels:
        ipx = np.argmin(np.abs(wl_by_py[row["band"]] - row["lambda"]))
        fiducial_bandpass[row["Channel"]] = {}
        (
            fiducial_bandpass[row["Channel"]]["wl"],
            fiducial_bandpass[row["Channel"]]["t"],
        ) = Instrument.pixel_to_band(1024, ipx, sparse=True, array=row["band"])
        output_lambdas.append(row["lambda"])

    # precompute spectral weights
    spec_weight = {}
    for source in np.unique(Primary_Catalog["SOURCE_ID"]):
        insrc = Primary_Catalog["SOURCE_ID"] == source
        D = Primary_Catalog[insrc]
        wl = D["WAVELENGTH"]
        det = D["DETECTOR"]
        px = D["X"]
        py = D["Y"]
        spec_weight[source] = np.zeros(len(wl))
        # loop over spectral channels
        for row in Channels:
            bandcenter = interpolate.interp1d(
                fiducial_bandpass[row["Channel"]]["wl"],
                fiducial_bandpass[row["Channel"]]["t"],
                fill_value="extrapolate",
            )
            inchannel = (
                (det == row["band"])
                & (wl >= row["lambda_min"])
                & (wl < row["lambda_max"])
            )
            band_wl, t = Instrument.pixel_to_band(
                px[inchannel], py[inchannel], sparse=True, array=row["band"]
            )
            tbs = np.reshape(bandcenter(band_wl.flatten()), t.shape)
            we = np.sum(t * tbs, axis=1) / np.sum(t * t, axis=1)
            spec_weight[source][inchannel] = we

    # figure out which observations are in the deep field
    indeep = np.zeros(len(Primary_Catalog), dtype="int8")
    if pointing_table is not None:
        ii = (pointing_table[Primary_Catalog["IMAGE_ID"]]["Flag"] == "deep_north") | (
            pointing_table[Primary_Catalog["IMAGE_ID"]]["Flag"] == "deep_south"
        )

        indeep[ii] = 1

    # this is where the data will be stored
    rows = []

    # get all the unique sources in the catalog
    sources = np.unique(Primary_Catalog["SOURCE_ID"])

    if source_id_lookup is not None:
        source_num = source_id_lookup
    else:
        source_num = dict(zip(sources, np.arange(len(sources))))

    source_idx = np.array([source_num[i] for i in Primary_Catalog["SOURCE_ID"]])

    for ii, isrc in enumerate(np.unique(source_idx)):
        srcdata = {}
        source = sources[ii]

        # start with data that is just one value per row
        srcdata["source_id"] = isrc

        if external_survey_lookup is not None:
            srcdata["external_survey"] = external_survey_lookup[isrc]
        else:
            srcdata["external_survey"] = 0

        if external_source_id_lookup is not None:
            srcdata["external_source_id"] = external_source_id_lookup[isrc]
        else:
            srcdata["external_source_id"] = 0

        # assign appropriate spherex_class for cosmology and ices:
        # Bit Name
        #   0 ASTROMETRIC
        #   1 PSF
        #   2 PHOTOMETRIC
        #   3 CALSPEC
        #   4 BRIGHT
        #   5 COSMOLOGY
        #   6 ICES
        if spherex_class_lookup is not None:
            srcdata["spherex_class"] = spherex_class_lookup[isrc]
        else:
            srcdata["spherex_class"] = 0

        if pm_ra is not None:
            srcdata["pm_ra"] = pm_ra
        else:
            srcdata["pm_ra"] = 0.0

        if pm_dec is not None:
            srcdata["pm_dec"] = pm_dec
        else:
            srcdata["pm_dec"] = 0.0

        if parallax is not None:
            srcdata["parallax"] = parallax
        else:
            srcdata["parallax"] = 0.0

        srcdata["ext_flg"] = False

        for col in list_items:
            srcdata[col] = []

        srcdata["lambda"] = output_lambdas

        # here's ones that are empty for now:
        srcdata["var_ext_bright"] = [np.nan] * len(output_lambdas)

        source_rows = Primary_Catalog["SOURCE_ID"] == source
        C = Primary_Catalog[source_rows]

        srcdata["ra"] = np.average(C["RA"])
        srcdata["dec"] = np.average(C["DEC"])

        wl = (C["WAVELENGTH"].to(pa_units["lambda"])).value
        isort = np.argsort(wl)
        wl = wl[isort]
        flux = (C["FLUX"][isort].to(pa_units["flux_allsky"])).value
        det = C["DETECTOR"][isort]
        err = (C["FLUX_ERR"][isort].to(pa_units["flux_allsky"])).value
        spec_wt = spec_weight[source][isort]
        weight = spec_wt**2 / (err) ** 2

        indeep_src = indeep[source_rows][isort]
        # do we a good number of deep observations of this source
        if np.sum(indeep_src) > 100:
            surveys = ["allsky", "deepfield"]
        else:
            surveys = ["allsky"]
            srcdata["flux_deepfield"] = [np.nan] * len(output_lambdas)
            srcdata["flux_err_deepfield"] = [np.nan] * len(output_lambdas)
            srcdata["unobs_flg_deepfield"] = [True] * len(output_lambdas)

        knots = np.sort(np.concatenate((Channels["lambda_min"], Channels["lambda"])))
        for survey in surveys:
            # add spectral weight here
            if method == "spline":
                if survey == "deepfield":
                    igood = indeep_src == 1
                else:
                    igood = indeep_src == 0
                Spline = interpolate.LSQUnivariateSpline(
                    wl[igood], flux[igood], knots, w=weight, k=3
                )
            sec = []
            sec_error = []
            unobs_flg = []
            for row in Channels:
                inchannel = (
                    (det == row["band"])
                    & (wl >= row["lambda_min"])
                    & (wl < row["lambda_max"])
                )
                if survey == "deepfield":
                    inchannel &= indeep_src == 1
                else:
                    inchannel &= indeep_src == 0
                n = len(flux[inchannel])
                if n > 0:
                    if method == "spline":
                        sec.append(Spline["lambda"])
                    elif method == "poly":
                        p = Polynomial.fit(
                            wl[inchannel],
                            flux[inchannel],
                            1,
                            w=np.sqrt(weight[inchannel]),
                        )
                        sec.append(p(row["lambda"]))
                    else:
                        sec.append(
                            np.average(flux[inchannel], weights=weight[inchannel])
                        )

                    residuals = flux[inchannel] - sec[-1]
                    if fluxerr_from_weights:
                        sec_error.append(
                            np.sqrt(1 / (np.sum(np.power(err[inchannel], -2))))
                        )
                    else:
                        sec_error.append(np.std(residuals) / np.sqrt(n))
                    unobs_flg.append(False)
                else:
                    sec.append(np.nan)
                    sec_error.append(np.nan)
                    unobs_flg.append(True)

            srcdata[f"flux_{survey}"] = sec
            srcdata[f"flux_err_{survey}"] = sec_error
            srcdata[f"unobs_flg_{survey}"] = unobs_flg

        sec = []
        sec_error = []
        flg = []
        for synband in synbands:
            bandweights = synband_interpolator[synband](wl)
            errweight = bandweights / err**2 / bandweights.sum()
            sec.append(np.sum(flux * bandweights) / bandweights.sum())
            sec_error.append(1.0 / np.sqrt(np.sum(errweight)))
            flg.append(np.nan)
        srcdata["synflux"] = sec
        srcdata["synflux_err"] = sec_error
        srcdata["var_ext_faint"] = flg

        rows.append(srcdata)

    # function to create field with metadata
    def create_pa_fld(name, pa_type, unit, descr):
        # all columns should have description
        fld_metadata = {"description": descr}
        if unit is not None:
            fld_metadata["unit"] = str(unit)
        return pa.field(name, pa_type, metadata=fld_metadata)

    schema = pa.schema([create_pa_fld(t[0], t[1], t[2], t[3]) for t in pa_flds_info])

    # Create pyarrow table with schema and rows
    allsky_cat = pa.Table.from_pylist(rows, schema=schema)

    with parquet.ParquetWriter(output_path, schema=allsky_cat.schema) as writer:
        writer.write_table(allsky_cat)


def calc_zodi_true_flux(Catalog, obstab, pix2band, zodi_model, wl_unit):
    # wls = catalog["WAVELENGTH"] * catalog["WAVELENGTH"].unit
    coos = SkyCoord(obstab["RA"], obstab["DEC"], frame=ICRS()).geocentrictrueecliptic
    times = Time(obstab["JD"], format="jd")

    zodiname = zodi_model.__class__.__name__
    if zodiname == "ModifiedKelsallModelWithHPFT":
        suns = get_body("sun", times).geocentrictrueecliptic
        obss = iter(int, None)  # dummy iterator to give None
    elif zodiname == "ModifiedKelsallModel":
        suns = iter(int, None)  # dummy iterator to give None
        obss = get_body("earth", times).heliocentrictrueecliptic
    else:
        # FIXME: If new model is introduced in the future...
        raise ValueError(f"Unknown zodi model: {zodiname}")

    zodi = []
    true_fluxes = []
    # for row in tqdm(spherex_catalog):
    for row, coo, time, obspos, sunpos in zip(obstab, coos, times, obss, suns):
        wl = row["WAVELENGTH"] << wl_unit

        # Calculate the zodi background at this position, time, and wavelength
        zodi.append(zodi_model(time, coo, wl, obspos=obspos, sunpos=sunpos))

        # Get the bandpass of the pixel where this landed
        wls, trans = pix2band(row["X"], row["Y"], array=row["DETECTOR"], sparse=True)

        # integrate the spectrum of the source over the SPHEREx bandpass
        _int = np.sum(trans * Catalog.interp[row["SOURCE_ID"]](wls)) / np.sum(trans)

        # Let's make sure we convert everything to milliJansky
        true_fluxes.append((_int * Catalog.flux_unit[row["SOURCE_ID"]]).to_value(u.mJy))

    return np.array(zodi), np.array(true_fluxes)


def calc_noises(
    img,
    nmc=500,
    nsamp_factor=1,
    noise_model=None,
    rdnoise_e=0,
    dark_current_model=None,
    dark_e=0,
):
    if nmc > 0:
        noise_shape = (nmc, *img.shape)
        _img = np.tile(img, (nmc, 1, 1))
    else:
        noise_shape = img.shape
        _img = img

    poisson_e = np.random.normal(size=noise_shape) * np.sqrt(_img * nsamp_factor)

    # compute read noise
    if noise_model is not None:
        rdnoise_e = noise_model(noise_shape, rdnoise_e)
    else:
        rdnoise_e = np.zeros(noise_shape)

    # compute dark current
    if dark_current_model is not None:
        dark_e = dark_current_model(noise_shape, dark_e)
    else:
        dark_e = np.zeros(noise_shape)

    return poisson_e, rdnoise_e, dark_e


def do_psf_phot(signal, noise, opt_phot_kernel, baseline, pix_sr, nmc=500):
    if nmc > 0:
        total_signals = np.tile(signal, (nmc, 1, 1)) + noise
        # get the optimal photometry from just one realization
        flux_aboves = total_signals - baseline
        flux_above = flux_aboves[0, :]

        # get error bars from spread in all realizations
        phot_kernel = np.tile(opt_phot_kernel, (nmc, 1, 1))
        fluxs = np.sum(flux_aboves * phot_kernel, axis=(1, 2)) * pix_sr
        dflux = np.std(fluxs)
    else:
        total_signal = signal + noise
        flux_above = total_signal - baseline
        dflux = np.sqrt(np.average((noise * noise), weights=opt_phot_kernel)) * pix_sr

    flux = np.sum(flux_above * opt_phot_kernel) * pix_sr

    return flux, dflux


def tractor_xy_cutout(cutout_center, oversample, xy_this, xyi, offset=0):
    this_x_or_y = (
        cutout_center - 1.0 + int((xy_this - xyi) * oversample) + 0.5 + offset
    ) / oversample - 0.5
    return this_x_or_y


class QuickCatalog(object):
    def __init__(
        self,
        Pointings,
        Instrument,
        Scene,
        oversample=5,
        Use_Tractor=False,
        return_fit=[],
        do_not_fit=[],
        subpixel_offset_x=0,
        subpixel_offset_y=0,
        PSF_measurement=None,
        spectral_channel_table=None,
        Logger=None,
    ):
        """
        The class constructor.

        Parameters
        ----------
        Pointings : SPHEREx_ObsSimulator.Pointings
            A class containing a survey plan of SPHEREx pointings.

        Instrument : SPHEREx_InstrumentSimulator.Instrument
            A class containing information about instrumental effects to add.

        Scene : SPHEREx_SkySimulator.Scene
            A class containing information on sky sources to simulate. The
            `QuickCatalog` class only calls the zodi model.

        oversample : int, optional
            The factor by which the generated cutouts are oversampled
            initially. This should match the PSF oversampling factor.
            The default is 5.

        Use_Tractor : bool, optional
            If `True`, a cutout is created for each source and Tractor is
            invoked to measure the photometry. Any neighboring sources in the
            input catalog are included in the cutout.

        return_fit : list
            If using Tractor, source names included in `return_fit` will have
            their fits returned (cutout and Tractor best-fit at each
            observation).

        do_not_fit : list
            If using Tractor, source names included in `do_not_fit` will not be
            included in the Tractor model, and their flux will not be fit.
            However, their flux *will* be included in the cutout for other
            sources, if they fall within the cutout region.

        subpixel_offset_x, subpixel_offset_y: int
            If using Tractor, the `subpixel_offset_x` and `subpixel_offset_y`
            specifies the offset, in integer oversampled pixels in x and y dim,
            respectively, by which to place the main source in the cutout,
            relative to scene center. Should be an integer value less than or
            equal to the oversample value. The idea is to allow tests of the
            effect of subpixel placement. Default is 0.

        PSF_measurement : SPHEREx_InstrumentSimulator.PSF, optional
            A PSF object to use for measuring the PSF. If `None` (default), the
            simulation PSF (``Instrument.PSF``) is used. Works only if
            `Use_Tractor` is `False`.

        spectral_channel_table : astropy.Table, optional.
            An astropy Table containing the definitions of the spectral
            channels.   If provided, the output catalog also includes
            spectral channel in which each observation occurred.
            The table must contain column names `Channel`, `lambda_min`,
            `lambda_max`, and `band`.

        Logger : SPHEREx_Logger, optional
            A place to write messages. The default is None.

        Returns
        -------
        None.

        """

        self.P = Pointings
        self.I = Instrument
        self.Scene = Scene
        self.Use_Tractor = Use_Tractor
        self.return_fit = return_fit
        if self.Use_Tractor and (not tractor_is_installed):
            raise ValueError("Tractor is not available")
        self.do_not_fit = do_not_fit
        self.subpixel_offset_x = subpixel_offset_x
        self.subpixel_offset_y = subpixel_offset_y

        self.oversample = oversample

        self.PSF_simulation = self.I.PSF

        if spectral_channel_table is None:
            self.spectral_channel_table = None
        else:
            self.spectral_channel_table = spectral_channel_table

        if PSF_measurement is None:  # Use the same as simulation
            self.PSF_measurement = copy.deepcopy(self.PSF_simulation)
            self._different_psfs = False
        else:
            self.PSF_measurement = copy.deepcopy(PSF_measurement)
            self._different_psfs = True

        self.logger = Logger if Logger else logging.getLogger()

        # calculate a WCS for each pointing.
        # for now, take a shortcut assuming that arrays 1,4 2,5 3,6 all use the same WCS
        self.logger.info("Constructing WCSArray for each detector")

        self.wcs_list = {}
        ra = self.P.pointing_table["ra"]
        dec = self.P.pointing_table["dec"]
        roll = self.P.pointing_table["psi2"]

        _pole = 90.0 - 1.0e-8  # subtract a small number to avoid pole singularity
        for array in [1, 2, 3]:
            wcs_at_pole = SPinst.get_spherex_wcs(0, _pole, array=array)
            wl = SPsky.WcsArray.from_lonlat(wcs_at_pole, ra, dec, roll)
            self.wcs_list[array] = wl

        # make a list of boresight coordinates
        self.boresight_list = SkyCoord(ra, dec, frame="icrs")

    def find_obs(
        self,
        name,
        coordinate,
        boresight_list=None,
        wcs_list=None,
        pointing_table=None,
        pix2band=None,
        max_distance=None,
        logger=None,
    ):
        """A simple wrapper of `find_obs` for QuickCatalog purpose."""
        return SPobs.sourcefinder.find_obs(
            name,
            coordinate,
            boresight_list=boresight_list or self.boresight_list,
            wcs_list=wcs_list or self.wcs_list,
            pointing_table=pointing_table or self.P.pointing_table,
            pix2band=pix2band or self.I.pixel_to_band,
            max_distance=max_distance or 8.0 * u.deg,
            logger=logger or self.logger,
        )

    def __call__(self, Catalog, nmc=500, cutout_size=20):
        """
        Build SPHEREx and truth catalogs from the given catalog of sources.

        Parameters
        ----------
        Catalog : Catalog_to_Simulate object
            The class containing the sources for which to generate simulated
            SPHEREx data.

        nmc : int, optional
            The number of noise realizations to generate when estimating
            flux error bars.  If set to zero, it obtains a local estimate of
            the noise from the simulated postage stamp. The default is 500.

        cutout_size : int optional
            When in Use_Tractor mode, this is the size of the cutout scene
            in SPHEREx pixel coordinates. The default is 20, with only sources
            in the central 10x10 region considered.

        Returns
        -------
        obstab : `astropy.table.Table`
            A table of source observations that is designed to look like the
            output of SPHEREx Level 3.
        trutab : `astropy.table.Table`
            A catalog containing the zodi signal and the true flux of the source
            for each observation.

        """

        ## Gemma added timing
        obstab = []
        trutab = None

        _u_wl = u.um

        # Loop over all the sources in the catalog
        for name in Catalog.Names:
            # find unique observations of this source
            _tab = self.find_obs(name, Catalog.Coordinates[name])
            obstab.append(_tab)
            self.logger.info(
                f"Found {len(_tab)} observations of {name} for the catalog."
            )
        obstab = vstack(obstab)
        obstab["FLUX"] = np.ones(len(obstab), dtype="float64")
        obstab["FLUX_ERR"] = np.ones(len(obstab), dtype="float64")
        obstab["UNIQUE_ID"] = np.arange(len(obstab), dtype="int64")


        # Setup columns

        for _c, _u, _d in (
            ("RA", u.deg, "RA (ICRS) of source"),
            ("DEC", u.deg, "DEC (ICRS) of source"),
            ("X", u.pix, "Pixel x coordinate of source"),
            ("Y", u.pix, "Pixel y coordinate of source"),
            ("WAVELENGTH", _u_wl, "Central response wavelength of this measurement"),
            ("UNIQUE_ID", None, "Unique ID of this photometry measurement"),
            ("FLUX", u.mJy, "Measured flux"),
            ("FLUX_ERR", u.mJy, "Measured flux error"),
            ("DETECTOR", None, "Detector array"),
            ("SOURCE_ID", None, "Source name"),
            ("IMAGE_ID", None, "Exposure"),
            ("JD", None, "Exposure time [JD]"),
        ):
            if _u is not None:
                obstab[_c].unit = _u
            obstab[_c].description = _d

        # If using Tractor and best-fit models are requested, set up HDU list to hold them
        if self.Use_Tractor and self.return_fit:
            hdul_bestfits = fits.HDUList()
            extidx = 0
        
        start = time.time()

        # now set up the Truth Catalog
        z_MJysr, true_fluxes_mJy = calc_zodi_true_flux(
            Catalog,
            obstab,
            pix2band=self.I.pixel_to_band,
            zodi_model=self.Scene.zodi_model,
            wl_unit=_u_wl,
        )

        trutab = Table(
            data=(z_MJysr, true_fluxes_mJy),
            names=("Zodi", "Flux"),
            units=(u.MJy / u.sr, u.mJy),
            descriptions=(
                "Zodi surface brightness at the time, wavelength, and sky coord of this observation.",
                "Input source brightness integrated over SPHEREx bandpass",
            ),
        )
        trutab.add_column(obstab["UNIQUE_ID"], name="UNIQUE_ID")

        # Next simulate the SPHEREx photometric measurements

        # NOTE: If I use `mJy/sr` throughout __call__, some values becomes too
        # small, and I found the final result sometimes suffer from
        # non-negligible machine epsilon error, especially when final flux is
        # << 1mJy, but still I cannot understand why exactly. - ysBach
        # Precompute some factors related to the Instrument
        pix_sr = ((6.2 * u.arcsec) ** 2).to_value(u.sr)  # solid angle (Omega_pixel)
        e_per_MJysr = (self.I.Mjysr_to_e_per_s * self.I.t_int).to_value(
            u.electron / (u.MJy / u.sr)
        )
        _srcpix2sky = self.oversample**2 / pix_sr * 1.0e-9  # in MJy/sr


        if self.I.noise_model is not None:
            rdn_e = self.I.sigma_read.to_value(u.electron)
        else:
            rdn_e = [0.0] * 6

        if self.I.dark_current_model is not None:
            dk_e = (self.I.dark_t_int).to_value(u.electron)
        else:
            dk_e = [0.0] * 6

        if self.Use_Tractor:
            # First be sure to set nmc=0
            nmc = 0
            MJysr_per_e_sq = (1 / e_per_MJysr) ** 2
            rdn_sq = rdn_e**2
            cutout_size_oversample = cutout_size * self.oversample
            cutout_center = cutout_size_oversample // 2 + 1
            xobss = obstab["X"].data
            yobss = obstab["Y"].data
            imids = obstab["IMAGE_ID"].data
            unids = obstab["UNIQUE_ID"].data
            dets = obstab["DETECTOR"].data

        measured_flux = []
        measured_flux_err = []

        # counting, for debugging purposes
        obs_count = 0

        # print("obstab column names", obstab.colnames)
        # print("Length of obs table = ", len(obstab))
        for row, z_true_MJysr, f_true_mJy in zip(obstab, z_MJysr, true_fluxes_mJy):
            srcid = row["SOURCE_ID"]
            if srcid not in self.do_not_fit:
                array = row["DETECTOR"]
                xi = row["X"]
                yi = row["Y"]
                imgid = row["IMAGE_ID"]
                unqid = row["UNIQUE_ID"]
                # get the PSF at this location
                kernel = self.PSF_simulation.psf(xi, yi, array=array)

                # -------------------------------------------------------
                if self.Use_Tractor:
                    # Create a cutout for Tractor, including neighboring sources (if any)
                    kernel_tractor = kernel.T
                    # Transpose necessary to work on numpy array
                    kernel_off_x = int(kernel_tractor.shape[0] / 2.0)
                    kernel_off_y = int(kernel_tractor.shape[1] / 2.0)

                    # Get all sources within 5 SPHEREx pixels of this one on this exposure
                    dxs = xobss - xi
                    dys = yobss - yi
                    m_dx5 = np.abs(dxs) < 5.0
                    m_dy5 = np.abs(dys) < 5.0
                    m_imid = imids == imgid
                    m_det = dets == array
                    m_unid = unids != unqid
                    m_wo_unid = m_dx5 & m_dy5 & m_imid & m_det
                    idxclose = np.where(m_wo_unid & m_unid)

                    # Create hires cutout.
                    # Only sources in central 10x10 SPHEREx pixel region will be photometered.
                    cutout_full = np.zeros(
                        (cutout_size_oversample, cutout_size_oversample)
                    )
                    # Remove the central main source, for background calculation,
                    # assuming all sub-threshold sources nearby, no other photometered refcat sources.
                    cutout_no_primary = cutout_full.copy()

                    # Put the main source at the center, plus any specified subpixel offset
                    xlow = cutout_center + self.subpixel_offset_x - kernel_off_x
                    xhigh = cutout_center + self.subpixel_offset_x + kernel_off_x
                    ylow = cutout_center + self.subpixel_offset_y - kernel_off_y
                    yhigh = cutout_center + self.subpixel_offset_y + kernel_off_y

                    cutout_full[ylow:yhigh, xlow:xhigh] += kernel_tractor * f_true_mJy

                    # Now step through any neighboring sources and drop those
                    # into the oversampled cutout at the appropriate
                    # position(s)
                    if idxclose[0].size != 0:
                        for idx in idxclose[0]:
                            # Compute x/y offsets (oversampled, approximate)
                            xoff = int(dxs[idx] * self.oversample)
                            yoff = int(dys[idx] * self.oversample)

                            xlow = cutout_center + xoff - kernel_off_x
                            xhigh = cutout_center + xoff + kernel_off_x
                            ylow = cutout_center + yoff - kernel_off_y
                            yhigh = cutout_center + yoff + kernel_off_y

                            cutout_full[ylow:yhigh, xlow:xhigh] += (
                                kernel_tractor * true_fluxes_mJy[idx]
                            )

                            cutout_no_primary[ylow:yhigh, xlow:xhigh] += (
                                kernel_tractor * true_fluxes_mJy[idx]
                            ) # with no primary source at the center

                    source_pixels = cutout_full
                    source_pixels_no_primary = cutout_no_primary

                    # downscale the cutout with no primary source
                    source_no_primary_pixels_MJysr = (
                        downscale_local_mean(
                            source_pixels_no_primary, (self.oversample, self.oversample)
                        )
                        * _srcpix2sky
                    )

                    # compute the background due to sub-threshold sources, around the central area
                    sub_x_l = int(source_no_primary_pixels_MJysr.shape[0] / 2) - 3
                    sub_x_h = int(source_no_primary_pixels_MJysr.shape[0] / 2) + 4
                    sub_y_l = int(source_no_primary_pixels_MJysr.shape[1] / 2) - 3
                    sub_y_h = int(source_no_primary_pixels_MJysr.shape[1] / 2) + 4
                    baseline_faint_MJysr = np.mean(
                        source_no_primary_pixels_MJysr[sub_y_l:sub_y_h, sub_x_l:sub_x_h]
                        )
                else:
                    # multiply by the Truth catalog flux
                    source_pixels = kernel * f_true_mJy

                # Hand it back to main routine for downscaling / adding noise etc.
                # -------------------------------------------------------

                # downgrade to SPHEREx resolution
                # convert to surface brightness units given the SPHEREx pixel size
                source_pixels_MJysr = (
                    downscale_local_mean(
                        source_pixels, (self.oversample, self.oversample)
                    )
                    * _srcpix2sky
                )


                # convert to photocurrent and total electrons
                cal = e_per_MJysr[array - 1]

                # add zodi
                signal_pixels_MJysr = source_pixels_MJysr + z_true_MJysr
                signal_pixels_e = signal_pixels_MJysr * cal

                photon_noise_e, rdnoise_e, dark_e = calc_noises(
                    signal_pixels_e,
                    nmc=nmc,
                    nsamp_factor=self.I.variance_factor,
                    noise_model=self.I.noise_model,
                    rdnoise_e=rdn_e[array - 1],
                    dark_current_model=self.I.dark_current_model,
                    dark_e=dk_e[array - 1],
                )

                # get photometry and error
                # sum noise and calibrate back to sky signal units
                total_noise_MJysr = (rdnoise_e + photon_noise_e + dark_e) / cal

                perfect_baseline_MJysr = z_true_MJysr + np.mean(total_noise_MJysr)
                
                if self.Use_Tractor:
                    # total baseline, including effects due to sub-threshold sources
                    perfect_baseline_MJysr = perfect_baseline_MJysr + baseline_faint_MJysr


                # ## double check background subtraction
                # if obs_count == 0:
                #     fig = plt.figure(figsize=(14,9))
                #     plt.subplot(2,3,1)
                #     plt.imshow(cutout_full)
                #     plt.title("Cutout Full", fontsize=13)
                #     plt.colorbar()

                #     plt.subplot(2,3,2)
                #     plt.imshow(cutout_no_primary)
                #     plt.title("Cutout, No Primary", fontsize=13)
                #     plt.colorbar()

                #     plt.subplot(2,3,3)
                #     plt.plot(source_no_primary_pixels_MJysr.flatten(), 'o')
                #     plt.axhline(baseline_faint_MJysr, label='faint baseline')
                #     plt.ylabel("downsample, no primary, noiseless MJy/Sr, flattened", fontsize=13)
                #     plt.legend()

                #     plt.subplot(2,3,4)
                #     plt.imshow(signal_pixels_MJysr + total_noise_MJysr - perfect_baseline_MJysr)
                #     plt.colorbar()
                #     plt.title("OG background subtracted noisy", fontsize=13)

                #     plt.subplot(2,3,5)
                #     plt.imshow(signal_pixels_MJysr + total_noise_MJysr - perfect_baseline_MJysr_faint)
                #     plt.title("Background sub, with faint, noisy", fontsize=13)
                #     plt.colorbar()

                #     plt.subplot(2,3,6)
                #     plt.plot((signal_pixels_MJysr + total_noise_MJysr - perfect_baseline_MJysr).flatten(), 'o', label='og')
                #     plt.plot((signal_pixels_MJysr + total_noise_MJysr - perfect_baseline_MJysr_faint).flatten(), 'o', label='new')
                #     plt.legend()
                #     plt.ylabel("to fit, flattened, MJy/Sr", fontsize=13)

                #     fig.tight_layout()
                #     plt.show()


                # -------------------------------------------------------
                if self.Use_Tractor:
                    # Perform the photometry with Tractor rather than optimal extraction

                    # The following gets all sources to be photometered including main source
                    idx_in_cutout = np.where(m_wo_unid)

                    # Build the source list for the cutout, but exclude any in the "do_not_fit" list
                    source_list = dict(ID=[], RA=[], DEC=[], X=[], Y=[])
                    for ii, _idx in enumerate(idx_in_cutout[0]):
                        cat_cutout = obstab[_idx]
                        srcid_this = cat_cutout["SOURCE_ID"]

                        if srcid_this not in self.do_not_fit:
                            thisra = cat_cutout["RA"]
                            thisdec = cat_cutout["DEC"]
                            x_this = cat_cutout["X"]
                            y_this = cat_cutout["Y"]

                            main_src = srcid_this == srcid
                            thisx = tractor_xy_cutout(
                                cutout_center,
                                self.oversample,
                                x_this,
                                xi,
                                offset=self.subpixel_offset_x if main_src else 0,
                            )
                            thisy = tractor_xy_cutout(
                                cutout_center,
                                self.oversample,
                                y_this,
                                yi,
                                offset=self.subpixel_offset_y if main_src else 0,
                            )
                            source_list["ID"].append(ii + 1)
                            source_list["RA"].append(thisra)
                            source_list["DEC"].append(thisdec)
                            source_list["X"].append(thisx)
                            source_list["Y"].append(thisy)

                    nsrc = len(source_list["ID"])
                    for key, val in source_list.items():
                        source_list[key] = np.array(val)
                    for key in ["SersicRadius", "SersicAB", "SersicNu", "SersicPhi"]:
                        source_list[key] = np.array([-1.0] * nsrc)

                    # Build the call to Tractor, modified to include baseline due to faint sources
                    cutout_to_fit_MJysr = (
                        signal_pixels_MJysr + total_noise_MJysr - perfect_baseline_MJysr
                    )

                    photon_variance = signal_pixels_e * self.I.variance_factor
                    cutout_variance = (
                        photon_variance + rdn_sq[array - 1] + dk_e[array - 1]
                    ) * MJysr_per_e_sq[array - 1]

                    _src_return = srcid in self.return_fit
                    _res = tractor_utils.tractor_fit_spherex(
                        cutout_to_fit_MJysr,
                        WCS(),
                        cutout_variance,
                        source_list=source_list,
                        array_number=array,
                        cutout_center=(xi, yi),
                        sp_inst=self.I,
                        return_fit=_src_return,
                        in_table=False,
                    )
                    _fluxres_MJy = _res[0] if _src_return else _res
                    _fluxres_mJy = _fluxres_MJy[:, 1:] * pix_sr * 1.0e9
                    # [:, 0] is mere "ID", which is not needed here

                    idx_main_source = np.where(
                        (
                            source_list["X"]
                            == (cutout_center - 1.0 + 0.5 + self.subpixel_offset_x)
                            / self.oversample
                            - 0.5
                        )
                        & (
                            source_list["Y"]
                            == (cutout_center - 1.0 + 0.5 + self.subpixel_offset_y)
                            / self.oversample
                            - 0.5
                        )
                    )[0]

                    if not len(idx_main_source) > 0:
                        raise ValueError("Main source not found in the cutout")
                    idx_main_source = idx_main_source[0]

                    flux_mJy = _fluxres_mJy[idx_main_source, 0]
                    dflux_mJy = _fluxres_mJy[idx_main_source, 1]

                    if _src_return:
                        bestfit = _res[1]
                        wli = row["WAVELENGTH"]

                        # Return the cutout
                        hdul_bestfits.append(fits.ImageHDU(cutout_to_fit_MJysr))
                        hdul_bestfits[extidx].header["OBJECT"] = "Cutout"
                        hdul_bestfits[extidx].header["WAVE"] = round(wli, 2)
                        hdul_bestfits[extidx].header["SOURCE"] = srcid
                        hdul_bestfits[extidx].header["IMAGE"] = imgid
                        extidx += 1

                        # Return Tractor best-fit model
                        hdul_bestfits.append(fits.ImageHDU(bestfit[0].data))
                        hdul_bestfits[extidx].header["OBJECT"] = "Tractor model"
                        hdul_bestfits[extidx].header["WAVE"] = round(wli, 2)
                        hdul_bestfits[extidx].header["SOURCE"] = srcid
                        hdul_bestfits[extidx].header["IMAGE"] = imgid
                        extidx += 1

                        # Return sources in model
                        rows = []

                        for jj, _idx in enumerate(idx_in_cutout[0]):
                            cat_this = obstab[_idx]
                            srcid_this = cat_this["SOURCE_ID"]
                            x_this = cat_this["X"]
                            y_this = cat_this["Y"]

                            # srcid_this == srcid: Main source, add any specified subpixel offset
                            main_src = srcid_this == srcid
                            thisx = tractor_xy_cutout(
                                cutout_center,
                                self.oversample,
                                x_this,
                                xi,
                                offset=self.subpixel_offset_x if main_src else 0,
                            )
                            thisy = tractor_xy_cutout(
                                cutout_center,
                                self.oversample,
                                y_this,
                                yi,
                                offset=self.subpixel_offset_y if main_src else 0,
                            )

                            # thisx = (cutout_center-1. +
                            # int((x_this-xi)*self.oversample) +
                            # 0.5)/self.oversample-0.5
                            # thisy = (cutout_center-1. +
                            # int((y_this-yi)*self.oversample) +
                            # 0.5)/self.oversample-0.5

                            if srcid_this in self.do_not_fit:
                                thisflux = -999.0
                                thisfluxerr = -999.0
                            else:
                                thisflux = _fluxres_mJy[jj, 0]
                                thisfluxerr = _fluxres_mJy[jj, 1]
                            rows.append(
                                [srcid_this, thisx, thisy, thisflux, thisfluxerr]
                            )

                        sources_in_model = Table(
                            rows=rows,
                            names=("SOURCE_ID", "X", "Y", "Flux", "Fluxerr"),
                            meta={"name": "tractor_in"},
                        )

                        hdul_bestfits.append(fits.BinTableHDU(sources_in_model))
                        hdul_bestfits[extidx].header["OBJECT"] = "Tractor sources"
                        hdul_bestfits[extidx].header["WAVE"] = round(wli, 2)
                        hdul_bestfits[extidx].header["SOURCE"] = srcid
                        hdul_bestfits[extidx].header["IMAGE"] = imgid
                        extidx += 1

                else:
                    if self.PSF_measurement.preloaded:
                        try:
                            opt_phot_kernel = self.PSF_measurement.psf_opt_phot(
                                xi, yi, array=array
                            )
                        except AttributeError:
                            raise AttributeError(
                                "PSF_measurement must have a method psf_opt_phot."
                                "It seems it is not preloaded - Try `PSF(preload_psf=True)`."
                            )
                    else:
                        if self._different_psfs:
                            # Overwrite kernel with the measurement PSF
                            kernel = self.PSF_measurement.psf(xi, yi, array=array)
                        opt_phot_kernel = SPinst.psf.get_phot_kernel(
                            kernel, self.oversample
                        )
                    flux_MJy, dflux_MJy = do_psf_phot(
                        signal=signal_pixels_MJysr,
                        noise=total_noise_MJysr,
                        baseline=perfect_baseline_MJysr,
                        opt_phot_kernel=opt_phot_kernel,
                        pix_sr=pix_sr,
                        nmc=nmc,
                    )
                    flux_mJy = flux_MJy * 1.0e9
                    dflux_mJy = dflux_MJy * 1.0e9

                # get the units right but then remove the units until later
                measured_flux.append(flux_mJy)
                measured_flux_err.append(dflux_mJy)

            else:  # Source in do_not_fit list
                measured_flux.append(-99.0)
                measured_flux_err.append(-99.0)

            obs_count += 1
        # -------------------------------------------------------
        obstab["FLUX"] *= np.array(measured_flux)
        obstab["FLUX_ERR"] *= np.array(measured_flux_err)


        # if spectral channel definitions are present, return which spectral
        # channel returned each measurement
        if self.spectral_channel_table is not None:
            obstab["SPEC_CHANNEL"] = np.zeros_like(obstab["DETECTOR"])
            obstab["SPEC_CHANNEL"].description = "Spectral channel of observation"
            wl = obstab["WAVELENGTH"]
            det = obstab["DETECTOR"]
            for row in self.spectral_channel_table:
                inchannel = (
                    (det == row["band"])
                    & (wl >= row["lambda_min"])
                    & (wl < row["lambda_max"])
                )
                obstab["SPEC_CHANNEL"][inchannel] = row["Channel"]

        # If in Tractor mode and fits are requested,return HDU list that has them
        if (self.Use_Tractor) & (len(self.return_fit) > 0):
            return obstab, trutab, hdul_bestfits
        else:
            return obstab, trutab


class Catalog_to_Simulate(object):
    def __init__(self, Logger=None):
        """
        Instatiate a catalog of sources to simulate.

        Returns
        -------
        None.

        """
        self.Names = []
        self.Coordinates = {}
        self.Spectra = {}
        self.interp = {}
        self.flux_unit = {}
        self.nsources = 0
        if Logger:
            self.logger = Logger
        else:
            self.logger = logging.getLogger()

    def load_catalog(
        self,
        inputpath,
        band_data_file="photometric_bands.fits",
    ):
        """


        Parameters
        ----------
        inputpath : string
            DESCRIPTION.

        band_data_file : string, optional
            a . The default is 'photometric_bands.fits'.

        Returns
        -------
        None.

        """
        Bandinfo = Table.read(band_data_file)
        # Make a lookup of band name to wavelength
        band_wl = {}
        for ii, field in enumerate(Bandinfo["band"]):
            band_wl[field] = Bandinfo["wl"][ii]

        T = Table.read(inputpath)
        for row in T:
            s_rows = []
            for band in band_wl.keys():
                if band in row.colnames:
                    if not np.isnan(row[band]):
                        s_rows.append([band_wl[band], row[band] / 1e3])
            name = f'{row["source_id"]}'
            self.Names.append(name)
            self.Coordinates[name] = SkyCoord(
                row["ra"] * u.deg, row["dec"] * u.deg, frame="icrs"
            )
            spectrum = Table(rows=s_rows, names=["lambda", "FLUX"])
            spectrum.sort("lambda")
            spectrum["lambda"].unit = u.um
            spectrum["FLUX"].unit = u.mJy
            self.Spectra[name] = spectrum
            self.flux_unit[name] = u.mJy

            self.interp[name] = interpolate.interp1d(
                self.Spectra[name]["lambda"],
                self.Spectra[name]["FLUX"],
                kind="linear",
                bounds_error=False,
                fill_value=0.0,
            )

            self.logger.info(
                f"Added source {name} with coordinate {self.Coordinates[name]}. Spectrum is in {self.flux_unit[name]}"
            )

    def load_single(self, name, ra, dec, inputpath):
        """


        Parameters
        ----------
        name : TYPE
            DESCRIPTION.
        ra : TYPE
            DESCRIPTION.
        dec : TYPE
            DESCRIPTION.
        inputpath : TYPE
            DESCRIPTION.

        Returns
        -------
        None.

        """
        self.Names.append(name)
        self.Coordinates[name] = SkyCoord(ra, dec, frame="icrs")
        self.Spectra[name] = Table.read(inputpath)
        self.interp[name] = interpolate.interp1d(
            self.Spectra[name]["lambda"],
            self.Spectra[name]["FLUX"],
            kind="linear",
            bounds_error=False,
            fill_value=0.0,
        )
        self.flux_unit[name] = self.Spectra[name]["FLUX"].unit
        # TODO: do better checking of the spectrum input, make sure its one we can convert to millijansky

        self.logger.info(
            f"Added source {name} with coordinate {self.Coordinates[name]}. Spectrum is in {self.flux_unit[name]}"
        )

    @u.quantity_input
    def load_single_from_arrays(
        self,
        name,
        ra,
        dec,
        wave: u.um,
        flux: u.mJy,
    ):
        """
        Parameters
        ----------
        name : TYPE
            DESCRIPTION.
        ra : float
            RA of target (ICRS).
        dec : float
            dec of target (ICRS).
        wave : float array
            high-resolution wavelength of input spectrum.
        flux : float array
            flux of spectrum

        Returns
        -------
        None.

        """

        self.Names.append(name)
        self.Coordinates[name] = SkyCoord(ra, dec, frame="icrs")
        self.Spectra[name] = Table([wave, flux], names=["lambda", "FLUX"])
        self.interp[name] = interpolate.interp1d(
            self.Spectra[name]["lambda"],
            self.Spectra[name]["FLUX"],
            kind="linear",
            bounds_error=False,
            fill_value=0.0,
        )
        self.flux_unit[name] = self.Spectra[name]["FLUX"].unit
        # TODO: do better checking of the spectrum input, make sure its one we can convert to millijansky

        self.logger.info(
            f"Added source {name} with coordinate {self.Coordinates[name]}. Spectrum is in {self.flux_unit[name]}"
        )

    def load_single_ISO(self, inputpath, name=None, ra=None, dec=None):
        """
        Load smoothed ISO spectrum from a FITS table file.

        This method loads smoothed ISO spectrum derived from the SWS Atlas by
        Sloan et al. 2003 [1]_.  The data are smoothed to remove most of the
        noise and most of the fringe-like pattern cased by over-resolving the
        spectra.

        Parameters
        ----------
        inputpath : str, file-like or `pathlib.Path`
            File or path to a file containing spectrum.

        name : str, optional
            Name of object.  If not given, take it from `OBJECT` card of FITS
            header.

        ra : float, optional
            R.A. of object in the unit of degree.  If not given, take it from
            `ATTRA` card of FITS header.

        dec : float, optional
            Declination of object in the unit of degree.  If not given, take it
            from `ATTDEC` card of FITS header.

        Returns
        -------
        None

        References
        ----------
        .. [1] G. C. Sloan et al., "A uniform database of 2.4--45.4 micron
        spectra from the *Infrared Space Observatory* Short Wavelength
        Spectrometer," The Astrophysical Journal Supplement Series, vol. 147,
        pp. 379-401, 2003.

        """

        hdr = fits.getheader(inputpath, ext=1)
        spectrum = Table.read(inputpath, format="fits", hdu=1)

        # set sky coordinates of object
        if ra is None:
            try:
                ra = hdr["ATTRA"]
            except KeyError:
                msg = (
                    f"No 'ATTRA' card in the file '{inputpath}'. "
                    "Do not load spectrum from it."
                )
                self.logger.warning(msg)
                return

        if dec is None:
            try:
                dec = hdr["ATTDEC"]
            except KeyError:
                msg = (
                    f"No 'ATTDEC' card in the file '{inputpath}'. "
                    "Do not load spectrum from it."
                )
                self.logger.warning(msg)
                return

        skycrd = SkyCoord(ra, dec, unit=u.deg, frame="icrs")

        # set name of object
        if name is None:
            try:
                name = hdr["OBJECT"].replace(" ", "_")
            except KeyError:
                name = f"ISOJ{skycrd.ra.deg:07.3f}{skycrd.dec.deg:+06.3f}"
                msg = (
                    f"No 'OBJECT' card in the file '{inputpath}'. "
                    f"Use '{name}' as the name of object."
                )
                self.logger.warning(msg)

        if name in self.Names:
            msg = (
                f"Spectrum of object '{name}' already loaded. "
                f"Do not load spectrum from '{inputpath}'."
            )
            self.logger.warning(msg)
            return

        self.Names.append(name)
        self.Coordinates[name] = skycrd

        # set spectrum, after converting unit of `flux` column to mJy and
        # `wave` to um for convenience
        try:
            spectrum["flux"] *= spectrum["flux"].unit.to(u.mJy)
        except u.UnitConversionError:
            msg = (
                f"Unit ('{spectrum['flux'].unit}') of 'flux' column not "
                f"convertible to 'mJy'.  Do not load spectrum from '{inputpath}'."
            )
            self.logger.warning(msg)
            return
        spectrum["flux"].unit = u.mJy

        try:
            spectrum["wave"] *= spectrum["wave"].unit.to(u.um)
        except u.UnitConversionError:
            msg = (
                f"Unit ('{spectrum['wave'].unit}') of 'wave' column not "
                f"convertible to 'um'.  Do not load spectrum from '{inputpath}'."
            )
            self.logger.warning(msg)
            return
        spectrum["wave"].unit = u.um

        self.Spectra[name] = spectrum

        # set interpolator object
        self.interp[name] = interpolate.interp1d(
            spectrum["wave"].to(u.um).value,
            spectrum["flux"].data,
            kind="linear",
            bounds_error=False,
            fill_value=0.0,
        )

        # set unit of flux returned by interpolator
        self.flux_unit[name] = spectrum["flux"].unit

        self.logger.info(
            "Add source '%s' with coordinate %s. Spectrum is in %s.",
            name,
            skycrd,
            self.flux_unit[name],
        )

        return


def make_check_plot(
    name, Sources_to_Simulate, SPHEREx_Catalog, Truth_Catalog, filename
):
    import matplotlib.pyplot as plt
    from astropy.table import hstack

    plt.gcf().set_size_inches((16, 5))
    plt.plot(
        Sources_to_Simulate.Spectra[name]["lambda"],
        Sources_to_Simulate.Spectra[name]["FLUX"],
        label="Source Spectrum",
    )
    T = hstack(
        [SPHEREx_Catalog["FLUX", "FLUX_ERR", "WAVELENGTH"], Truth_Catalog["Flux"]]
    )[SPHEREx_Catalog["SOURCE_ID"] == name]
    plt.plot(T["WAVELENGTH"], T["Flux"], "go", label="SPHEREx prediction")
    plt.errorbar(
        T["WAVELENGTH"],
        T["FLUX"],
        yerr=T["FLUX_ERR"],
        fmt="o",
        color="red",
        label="SPHEREx measurement",
    )

    plt.title(name)
    plt.xlabel(f"$\lambda$ [{Sources_to_Simulate.Spectra[name]['lambda'].unit}]")
    plt.ylabel(f"Flux  [{Sources_to_Simulate.Spectra[name]['FLUX'].unit}]")
    plt.legend()
    if filename is not None:
        plt.savefig(filename)
        plt.clf()
    else:
        plt.show()


if __name__ == "__main__":
    # start a logger
    Logger = SPHEREx_Logger(filename=None, mpi_rank=0, debug=False)

    Logger.info("Starting QuickCatalog demo")

    # Set up an example catalog
    Sources_to_Simulate = Catalog_to_Simulate(Logger=Logger)
    Sources_to_Simulate.load_single(
        "BD29+2091",
        161.84651041666663 * u.deg,
        28.398868888888888 * u.deg,
        "../BD29+2091.fits",
    )

    Sources_to_Simulate.load_single(
        "WD1057+719", 165.14267917 * u.deg, 71.63414444 * u.deg, "../wd1057_719.fits"
    )

    Sources_to_Simulate.load_catalog("example_catalog.fits")

    # Get a SPHEREx Survey Plan
    survey_plan_file = "../data/spherex_survey_plan_2_26_20_draft_alpha_6month.fits"
    Logger.info(f"Reading SPHEREx survey plan from {survey_plan_file}")
    SPHEREx_Pointings = SPobs.Pointings(
        input_file=survey_plan_file, Gaussian_jitter=0.0, roll_angle="psi2"
    )

    # Get an SPHEREx Instrument
    Logger.info("Setting up SPHEREx Instrument model")
    SPHEREx_Instrument = SPinst.Instrument(
        psf="../data/psf/simulated_PSF_database.fits",
        psf_downsample_by_array={1: 4, 2: 4, 3: 4, 4: 2, 5: 2, 6: 2},
        psf_trim_by_array={1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32},
        noise_model=SPinst.white_noise,
        dark_current_model=SPinst.poisson_dark_current,
        lvf_model=SPinst.smile_lvf,
    )

    # Set up a sky scene in order to initialize a background model
    Logger.info("Setting up Sky Scene with background model.")
    Scene = SPsky.Scene(
        SPHEREx_Pointings, zodi_model=SPsky.zodicalc.ModifiedKelsallModel()
    )

    # Initialize the Quick Catalog class
    QC = QuickCatalog(SPHEREx_Pointings, SPHEREx_Instrument, Scene, Logger=Logger)

    SPHEREx_Catalog, Truth_Catalog = QC(Sources_to_Simulate)

    make_check_plot(
        "BD29+2091", Sources_to_Simulate, SPHEREx_Catalog, Truth_Catalog, "tmp.png"
    )

    make_check_plot(
        "WD1057+719", Sources_to_Simulate, SPHEREx_Catalog, Truth_Catalog, "tmp3.png"
    )

    make_check_plot(
        "4093772061286341376",
        Sources_to_Simulate,
        SPHEREx_Catalog,
        Truth_Catalog,
        "tmp4.png",
    )
